# Crafting Interpreters - Robert Nystrom

## IR research terms:
- Control flow graphs
- static single-assignment
- continuation-passing style
- three-address code

## Optimization research terms:
- Constant propagation
- common subexpression elimination
- loop invariant code motion
- global value numbering
- strength reduction
- scalar replacement of aggregates
- dead code elimination
- loop unrolling

## Single-pass compilers
Interleave parsing, analysis and code generation so that they produce output code directly in the parser, without ever allocating
any syntax trees or other IRs. They restrict the design of the language, because you have no intermediate data structures to store global info
about the program, and you dont revisit any previously parsed part of the code. That means that as soon as you see an expression,
you need to know enough to correctly compile it.
C was designed around this limitation (and thats why you cant call a function above the code that defines it)

Syntax-directed translation is a structured technique for building these all-at-once compilers. You associate an action with each 
piece of the grammar, usually one that generates output code. Then, whenever the parser matches that chunk of syntax, it executes the action,
building up the target code one rule at a time.

## Tree-walk Interpreters

Begin executing the code right after parsing it to an AST. To run the program, the interpreter traverses the AST one branch and leaf at a time,
evaluating each node as it goes.

This implementation style is common for student projects and little languages, since it tends to be slow.

## Transpilers

This is basically writing a front end for your language, and then in the backend, instead of doing all the work to lower the
semantics to some primitive target language, we produce a valid source code for some other language that is about as high level as ours. Then
we compile that language code. THis is called a source-to-source compiler or a transcompiler or transpiler.

## Just-in-time compilation (JIT)

This is not really a shortcut, and here whe want to know what architecture the users machine support. On the end users machine, when the program
is loaded - either from source or platform-independent bytecode - you compile it to native code for the architecture their computer supports
The most sophisticated JITs insert profiling hooks into the generated code to see which regions are most performance critical and what kind of 
data is flowing through them. Then, over time, they will automatically recompile those hotspots with more advanced optimizations.

## Difference between compilers and Interpreters

Compiling is an implementation technique that involves translating a source language to another - usually lower level form. Generating bytecode
or machine code is compiling, and transpiling to another language is also compiling. When we say a language implementation is a compiler, we mean 
it translates source code to some other form, but does not execute it. The user has to run it himself.

When we say an implementation is an interpreter, we mean it takes in source code and executes it immediately. It runs programs from source.

### First Chapter Challenges:
Q: Pick an open source implementation of a language you like. Download the source code and poke around in it. Try to find the code that implements the
scanner and parser. Are they handwritten, or generated using tools like Lex and Yacc?
A: Poked around with Kind (from HVM). The scanner (lexer) was generated by them using Rust, not using generation tools.

Q: Just-in-time compilation tends to be the fastest way to implement dynamically typed languages, but not all of them use it. What reasons are there to not JIT?

A: JIT can be hard to implement and to maintain. Like a native code compiler (which it is), it ties you to a specific CPU architecture.
Bytecode is generally more compact than machine code (since it's closer to the semantics of the language), so it takes up less memory. In platforms like embedded devices where memory may matter more than speed, that can be a worthwhile trade-off.
Some platforms, like iOS and most game consoles, expressly disallow executing code generated at runtime. The OS simply won't allow you to jump into memory that can be written to.

Q: Most Lips implementations that compile to C also contain an interpreter that lets them execute Lisp code on the fly as well. Why?

A: If we have macros (code that is executed at compile time), the compiler would not be able to do it, so the interpreter do. Compiling it before and then running it is lot overhead.

Paper: "A Unified Theory of Garbage Collection"
Paper: "The Next 700 Programming Languages"

Challenge -> Add support fort C-style /* ... */ block comments. Handle newlines in them. Consider allowing them to nest.
