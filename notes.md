# Crafting Interpreters - Robert Nystrom

## IR research terms:
- Control flow graphs
- static single-assignment
- continuation-passing style
- three-address code

## Optimization research terms:
- Constant propagation
- common subexpression elimination
- loop invariant code motion
- global value numbering
- strength reduction
- scalar replacement of aggregates
- dead code elimination
- loop unrolling

## Single-pass compilers
Interleave parsing, analysis and code generation so that they produce output code directly in the parser, without ever allocating
any syntax trees or other IRs. They restrict the design of the language, because you have no intermediate data structures to store global info
about the program, and you dont revisit any previously parsed part of the code. That means that as soon as you see an expression,
you need to know enough to correctly compile it.
C was designed around this limitation (and thats why you cant call a function above the code that defines it)

Syntax-directed translation is a structured technique for building these all-at-once compilers. You associate an action with each 
piece of the grammar, usually one that generates output code. Then, whenever the parser matches that chunk of syntax, it executes the action,
building up the target code one rule at a time.

## Tree-walk Interpreters

Begin executing the code right after parsing it to an AST. To run the program, the interpreter traverses the AST one branch and leaf at a time,
evaluating each node as it goes.

This implementation style is common for student projects and little languages, since it tends to be slow.

## Transpilers

This is basically writing a front end for your language, and then in the backend, instead of doing all the work to lower the
semantics to some primitive target language, we produce a valid source code for some other language that is about as high level as ours. Then
we compile that language code. THis is called a source-to-source compiler or a transcompiler or transpiler.

## Just-in-time compilation (JIT)

This is not really a shortcut, and here whe want to know what architecture the users machine support. On the end users machine, when the program
is loaded - either from source or platform-independent bytecode - you compile it to native code for the architecture their computer supports
The most sophisticated JITs insert profiling hooks into the generated code to see which regions are most performance critical and what kind of 
data is flowing through them. Then, over time, they will automatically recompile those hotspots with more advanced optimizations.

## Difference between compilers and Interpreters

Compiling is an implementation technique that involves translating a source language to another - usually lower level form. Generating bytecode
or machine code is compiling, and transpiling to another language is also compiling. When we say a language implementation is a compiler, we mean 
it translates source code to some other form, but does not execute it. The user has to run it himself.

When we say an implementation is an interpreter, we mean it takes in source code and executes it immediately. It runs programs from source.

### First Chapter Challenges:
Q: Pick an open source implementation of a language you like. Download the source code and poke around in it. Try to find the code that implements the
scanner and parser. Are they handwritten, or generated using tools like Lex and Yacc?
A: Poked around with Kind (from HVM). The scanner (lexer) was generated by them using Rust, not using generation tools.

Q: Just-in-time compilation tends to be the fastest way to implement dynamically typed languages, but not all of them use it. What reasons are there to not JIT?

A: JIT can be hard to implement and to maintain. Like a native code compiler (which it is), it ties you to a specific CPU architecture.
Bytecode is generally more compact than machine code (since it's closer to the semantics of the language), so it takes up less memory. In platforms like embedded devices where memory may matter more than speed, that can be a worthwhile trade-off.
Some platforms, like iOS and most game consoles, expressly disallow executing code generated at runtime. The OS simply won't allow you to jump into memory that can be written to.

Q: Most Lips implementations that compile to C also contain an interpreter that lets them execute Lisp code on the fly as well. Why?

A: If we have macros (code that is executed at compile time), the compiler would not be able to do it, so the interpreter do. Compiling it before and then running it is lot overhead.

Paper: "A Unified Theory of Garbage Collection"
Paper: "The Next 700 Programming Languages"

### Challenges -> Chapter 5: Representing Code 

1. expr -> expr ( "(" ( expr ("," expr)* )? ")" | "." IDENTIFIER )+ | IDENTIFIER | NUMBER 

Produce a grammar that matches the same language as the one above but does not use any of the notational sugar:

expr → expr calls
expr → IDENTIFIER
expr → NUMBER

calls → calls call
calls → call

call → "(" ")"
call → "(" arguments ")"
call → "." IDENTIFIER

arguments → expr
arguments → arguments "," expr

It is a function call.

2. The visitor pattern lets you emulate the functional style in an object oriented language. Devise a complementary pattern for a functional language. It should let you bundle all of the operations on one type together and let you define new types easily.

Maybe you could use pattern matching, or have a tuple with pointers where each pointer points to an operation for that type.

3. In Reverse Polish Notation (RPN), the operands to an arithmetic operator are both placed before the operator, so 1 + 2 becomes 1 2 +. Evaluation proceeds from left to right. Numbers are pushed onto an implicit stack. AN arithmetic operator pops the top two numbers, performs the operation and pushes the result. Thus, this:: (1 + 2) * (4 - 3)
in rpm becomse:
1 2 + 4 3 - *

Define a visitor class for our syntax tree classes that takes an expression, converts it to RPN and return the resulting string.

### Challenges -> Chapter 6 : Parsing Expressions

1. In C, a block is a statement form that allows you to pack a series of statements where a single one is expected. The comma operator is an analogous syntax for expressions. A comma-separated series of expressions can be given where a single expression is expected (except inside a function call's argument list). At runtime, the comma operator evaluates the left operand and discards the result. Then it evaluates and returns the right operand. Add support for comma expressions. Give them the same precedence and associativiy as in C. Write the grammar and then implement the necessary parsing code.

```
expression -> conditional;
conditional -> comma ( "?" expression ":" conditional)? ;
comma      -> equality ( "," equality )* ; // lowest precedence
equality -> comparison ( ( "!=" | "==" ) comparison )*;
comparison -> term ( ( ">" | ">=" | "<" | "<=" ) term)*;
term -> factor ( ( "-" | "+" ) factor )* ;
factor -> unary ( ( "/" | "*" ) unary)*;
unary -> ( "!" | "-") unary | primary;
primary -> NUMBER | STRING | "true" | "false" | "nil" | "(" expression ")";
           // Error productions...
           | ( "!=" | "==" ) equality
           | ( ">" | ">=" | "<" | "<=" ) comparison
           | ( "+" ) term
           | ( "/" | "*" ) factor ;
```

2. Likewise, add support for the C-style conditional or ternary operator ?: . What precedence level is allowed between the ? and :  ? Is the whole operator left-associative or right-associative.

3. Add error productions to handle each binary operator appearing without a left-hand operand. In other words, detect a binary operator appearing at the beginning of an expression. Report that as an error, but also parse and discard a right-hand operand with the appropriate precedence.

### Challenges -> Chapter 7 : Evaluating Expressions

1. Allowing comparisons on types other than numbers could be useful. The operators might have a reasonable interpretation for strings. Even comparisons among mixed types, like 3 < "pancake" could be handy to enable things like ordered collections of heterogeneous types. Or it could simply lead to bugs and confusion. Would you extend Lox to support comparing other types? If so, which pairs of types do you allow and how do you define their ordering? Justify your choices and compare them to other languages.

hmmm, maybe I would, but mainly just for strings (that we can calculate using ascii table or smth like this).

2. Many languages define + such that if either operand is a string, the other is converted to a string and the results are then concatenated. Ex: "scone" + 4 would yield scone4. Extend the code in visitBinaryExpr() to support that.

3. What happens right now if you divide a number by zero? What do you think should happen? Justify your choice. How do other languages you know handle division by zero and why do they make the choices they do? Change the implementation in visitBinaryExpr to detect and report a runtime error for this case.

Right now what happens is it returns infinity. I think its not "right" because it is undefined actually. I think it should return NaN. Many languages use NaN.


### Challenges -> Chapter 8 : Statements and state 

1. The REPL no longer supports entering a simple expression and automatically printing its result value. Add support to the REPL to let users type in both statements and expressions. If they enter a statement, execute it. If they enter an expression, evaluate it and display the result value.

2. Maybe you want lox to be a little more explicit about variable initialization. Instead of implicitly initializing variables to nil, make it a runtime error to access a variable that has not been initialized or assigned to, as in:
```
var a;
var b;

a = "assigned";
print a; // ok, was assigned First

print b; // error
```


3. What does the following program do?

```
var a  = 1;
{
  var a = a + 2;
  print a;
}
```

What did you expect it to do? Is it what you think it should do? What does analgous code in other languages you are familiar with do? What do you think users will expect this to do?


### Challenges -> Chapter 9 : Control flow

1. A few chapters from now, when Lox supports first-class functions and dynamic dispatch, we technically won't need branching statements built into the language. Show how conditional execution can be implemented in terms of those. Name a language that uses his technique for its control flow.

2. Likewise, looping can be implemented using those same tools, provided our interpreter supports an important optimization. What is it, and why is it necessary? Name a lang that uses it.

3. Unlike Lox, most other C-style languages also support break and continue statements inside loops. Add support for break statements.
The syntax is a break keyword followed by a semicolon. It should be a syntax error to have a break statement appear outisde of any enclosing loop. At runtime, a break statement causes execution to jump at the end of the nearest enclosing loop and proceeds from there. Note that the break may be nested inside other blocks and if statements that also need to be exited.

### Challenges -> Chapter 10 : Functions

1. Our interpreter carefully checks that the number of arguments passed to a function matches the number of parameters it expects. Since this check is done at runtime on every call, it has a performance cost. Smalltalk implementations don't have that problem. Why?

Smalltalk has different call syntax for different arities. To define a method that takes multiple arguments, you use keyword selectors. Each argument has a piece of the method name preceding instead of using commas as a separator. For example, a method like:

list.insert("element", 2)
To insert "element" as index 2 would look like this in Smalltalk:

list insert: "element" at: 2
Smalltalk doesn't use a dot to separate method name from receiver. More interestingly, the "insert:" and "at:" parts both form a single method call whose full name is "insert:at:". Since the selectors and the colons that separate them form part of the method's name, there's no way to call it with the wrong number of arguments. You can't pass too many or two few arguments to "insert:at:" because there would be no way to write that call while still actually naming that method.

2. Lox's function declaration syntax perform two independent operations. It creates a function and also binds it to a name. This improves usability for the common case where you do want to associate a name with the function. But in functional-styled code, you often want to create a function to immediately pass it to some other function or return it. In that case, it doesnt need a name. 
Languages that encourage a functional style usually support anonymous functions or lambdas - an expression syntax that creates a function without binding it to a name. Add anonymous function syntax to Lox so that this works:

```
fun thrice(fn) {
  for(var i = 1; i <= 3; i = i + 1) {
    fn(i);
  }
}

thrice(fun (a) {
  print a;
});
```

How do you handle the tricky case of an anonymous function expression occurring in an expression statement: `fun() {};` 

3. Is this valid?
`fun scope(a) { var a = "local"; }`
In other words, are a function's parameter in the same scope as its local variables or in an outer scope? What does Lox do? What do you think a language should do?
I personally think this shouldnt be valid, and if it is valid, make it shadowing. Ignore the arg and use the new local.

## Chapter 11 - Resolving and binding

1. Why is it safe to eagerly define the variable bound to a function's name when other variables must wait until after they are initialized before they can be used?

2. How do other languages you know handle local variables that refer to the same name in their initializer, like:
```
var a = "Outer";
{
 var a = a;
}
```

is it a runtime error? compile error? allowed? do they treat global variables differently? wdy think abt their choices.

3. Extend the resolver to report an error if a local variable is never used.

4. Our resolver calculates which environment the variable is found in, but it's still looked up by name in that map. A more efficient environment representation would store local variables in an array and look them up by index. 
Extend the resolver to associate a unique index for each local variable declared in a scope. When resolving a variable access, look up both the scope the variable is in and its index and store that. In the interpreter, use that to quickly access a variable by its index instead of using a map.
